{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/burutus1/bigData/blob/main/Course_HDFS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Для загрузки файла с HDFS и обработки данных согласно вашим требованиям, вам нужно использовать библиотеку `pyarrow`. Пожалуйста, установите ее с помощью команды:\n",
        "\n",
        "bash\n",
        "pip install pyarrow"
      ],
      "metadata": {
        "id": "MF0aBwTKZWv1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "id": "K-C1PxH5WYeh",
        "outputId": "f1c0646a-7cf6-467f-8d8e-9de939910848"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "__init__() takes at least 1 positional argument (0 given)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-9c74df92646b>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Создаем файловую систему HDFS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mfs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpyarrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHadoopFileSystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Чтение данных из файла на HDFS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyarrow/_hdfs.pyx\u001b[0m in \u001b[0;36mpyarrow._hdfs.HadoopFileSystem.__init__\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: __init__() takes at least 1 positional argument (0 given)"
          ]
        }
      ],
      "source": [
        "# python\n",
        "import pandas as pd\n",
        "import pyarrow.fs\n",
        "\n",
        "# Указываем путь к файлу на HDFS\n",
        "HDFS_PATH = 'hdfs://path_to_your_file/INPUT/rptDITRequestListOperativeData2018.csv'\n",
        "\n",
        "# Создаем файловую систему HDFS\n",
        "fs = pyarrow.fs.HadoopFileSystem()\n",
        "\n",
        "# Чтение данных из файла на HDFS\n",
        "with fs.open_input_file(HDFS_PATH) as f:\n",
        "    # Пропускаем 3 верхние строки\n",
        "    for _ in range(3):\n",
        "        next(f)\n",
        "\n",
        "    # Чтение данных в DataFrame, пропуская строки с разделителями ','\n",
        "    df = pd.read_csv(f, sep=',', skipinitialspace=True)\n",
        "\n",
        "# Удаление пропусков\n",
        "df = df.dropna()\n",
        "\n",
        "# Вывод загруженных данных\n",
        "print(df.head())\n",
        "\n",
        "# Сохранение датасета в файл на HDFS\n",
        "data.write.csv(\"hdfs://OUTPUT/rptDITRequestListOperativeData2018.csv\")\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.clustering import KMeans\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Создание сессии Spark\n",
        "spark = SparkSession.builder.appName('Clustering').getOrCreate()\n",
        "\n",
        "# Загрузка данных\n",
        "df = spark.read.csv('hdfs://OUTPUT/rptDITRequestListOperativeData2018.csv', header=True, inferSchema=True)\n",
        "\n",
        "# Создание вектора признаков\n",
        "features = df.columns\n",
        "assembler = VectorAssembler(inputCols=features, outputCol='features')\n",
        "data = assembler.transform(df)\n",
        "\n",
        "# Обучение модели K-means\n",
        "kmeans = KMeans(k=3, seed=1)\n",
        "model = kmeans.fit(data)\n",
        "\n",
        "# Добавление предсказанных кластеров к датасету\n",
        "predictions = model.transform(data)\n",
        "\n",
        "# Визуализация кластеров\n",
        "centers = model.clusterCenters()\n",
        "plt.scatter(centers[:, 0], centers[:, 1], c='red', marker='x')\n",
        "plt.show()\n",
        "\n",
        "# Оценка качества модели\n",
        "from pyspark.ml.evaluation import ClusteringEvaluator\n",
        "\n",
        "evaluator = ClusteringEvaluator()\n",
        "silhouette = evaluator.evaluate(predictions)\n",
        "print(\"Silhouette with squared euclidean distance = \" + str(silhouette))\n",
        "\n",
        "# Закрытие сессии Spark\n",
        "spark.stop()"
      ]
    }
  ]
}